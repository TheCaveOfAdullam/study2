{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "mount_file_id": "1mxktY7Z81WRGshCbng1s6E3ggwB3lsjl",
      "authorship_tag": "ABX9TyNmghNSOVYB5YJpMccvVCLr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheCaveOfAdullam/study2/blob/main/vibrationTest_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpTbpCIrBBZo",
        "outputId": "18ed431f-dbcc-4f5a-8772-8d468e5e96d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-R9019nAOHe",
        "outputId": "18821169-a4da-4fbb-de6e-d6e00c507c1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_model_optimization in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (0.1.8)\n",
            "Requirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (1.25.2)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (1.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.11.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_model_optimization\n",
        "!pip install h5py\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import psutil\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "import tensorflow_model_optimization as tfmot\n",
        "import h5py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 경로 설정\n",
        "base_dir = '/content/drive/MyDrive/ship_data'\n",
        "categories = ['normal', 'fault_BB', 'fault_RI', 'fault_SM']\n",
        "\n",
        "# 데이터 로드 및 전처리 함수 정의\n",
        "def load_data(base_dir, split):\n",
        "    X = []\n",
        "    y = []\n",
        "    split_dir = os.path.join(base_dir, split)\n",
        "    for category in categories:\n",
        "        category_dir = os.path.join(split_dir, category)\n",
        "        for file in os.listdir(category_dir):\n",
        "            file_path = os.path.join(category_dir, file)\n",
        "            data = pd.read_csv(file_path, header=None).values\n",
        "            data = pd.to_numeric(data.flatten(), errors='coerce').reshape(-1, data.shape[1])\n",
        "            data = np.nan_to_num(data).astype('float32')  # NaN 값을 0으로 대체하고, float32로 변환\n",
        "            X.append(data)\n",
        "            y.append(category)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# 데이터 로드\n",
        "X_train, y_train = load_data(base_dir, 'train')\n",
        "X_val, y_val = load_data(base_dir, 'validation')\n",
        "X_test, y_test = load_data(base_dir, 'test')\n",
        "\n",
        "# 데이터 차원 변경 (CNN 입력 형식에 맞게)\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_val = np.expand_dims(X_val, axis=-1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "\n",
        "# 레이블 인코딩\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_val_encoded = label_encoder.transform(y_val)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# 원-핫 인코딩\n",
        "y_train_categorical = to_categorical(y_train_encoded)\n",
        "y_val_categorical = to_categorical(y_val_encoded)\n",
        "y_test_categorical = to_categorical(y_test_encoded)"
      ],
      "metadata": {
        "id": "YAhhOF34ATLs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 CNN 모델 정의\n",
        "def create_model():\n",
        "    model = Sequential(name='CNN_Model')\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]), name='conv1'))\n",
        "    model.add(MaxPooling1D(pool_size=2, name='maxpool1'))\n",
        "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu', name='conv2'))\n",
        "    model.add(MaxPooling1D(pool_size=2, name='maxpool2'))\n",
        "    model.add(Flatten(name='flatten'))\n",
        "    model.add(Dense(100, activation='relu', name='dense1'))\n",
        "    model.add(Dropout(0.5, name='dropout'))\n",
        "    model.add(Dense(len(categories), activation='softmax', name='output'))\n",
        "    return model\n",
        "\n",
        "# 모델 생성 및 학습\n",
        "model = create_model()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train_categorical, epochs=10, batch_size=32, validation_data=(X_val, y_val_categorical))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADK64JO2ATOT",
        "outputId": "853165ff-faa7-473c-e104-46b9e98e664b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "219/219 [==============================] - 77s 347ms/step - loss: 1.6074 - accuracy: 0.4862 - val_loss: 0.9466 - val_accuracy: 0.5869\n",
            "Epoch 2/10\n",
            "219/219 [==============================] - 76s 347ms/step - loss: 0.7427 - accuracy: 0.6195 - val_loss: 0.3764 - val_accuracy: 0.8427\n",
            "Epoch 3/10\n",
            "219/219 [==============================] - 75s 342ms/step - loss: 0.5081 - accuracy: 0.7414 - val_loss: 0.1731 - val_accuracy: 0.9687\n",
            "Epoch 4/10\n",
            "219/219 [==============================] - 75s 345ms/step - loss: 0.4042 - accuracy: 0.8041 - val_loss: 0.1072 - val_accuracy: 0.9992\n",
            "Epoch 5/10\n",
            "219/219 [==============================] - 76s 346ms/step - loss: 0.3880 - accuracy: 0.8035 - val_loss: 0.1065 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "219/219 [==============================] - 76s 347ms/step - loss: 0.3641 - accuracy: 0.8202 - val_loss: 0.0894 - val_accuracy: 0.9992\n",
            "Epoch 7/10\n",
            "219/219 [==============================] - 76s 346ms/step - loss: 0.3366 - accuracy: 0.8347 - val_loss: 0.0940 - val_accuracy: 0.9851\n",
            "Epoch 8/10\n",
            "219/219 [==============================] - 75s 344ms/step - loss: 0.3346 - accuracy: 0.8508 - val_loss: 0.0888 - val_accuracy: 0.9945\n",
            "Epoch 9/10\n",
            "219/219 [==============================] - 75s 343ms/step - loss: 0.3307 - accuracy: 0.8549 - val_loss: 0.0531 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "219/219 [==============================] - 75s 345ms/step - loss: 0.3561 - accuracy: 0.8489 - val_loss: 0.1179 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x780fc045ba90>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 원래 모델 저장\n",
        "model.save('model.h5')"
      ],
      "metadata": {
        "id": "Kmj1N1jpYhxX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가지치기 함수 정의\n",
        "def prune_weights(model, pruning_percentage=0.8):\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, Dense):\n",
        "            weights, biases = layer.get_weights()\n",
        "\n",
        "            # 가중치의 절대값 기준으로 임계값 계산\n",
        "            abs_weights = np.abs(weights)\n",
        "            threshold = np.percentile(abs_weights, pruning_percentage * 100)\n",
        "\n",
        "            # 임계값 이하의 가중치를 0으로 설정\n",
        "            new_weights = np.where(abs_weights < threshold, 0, weights)\n",
        "\n",
        "            # 가지치기된 가중치로 레이어 설정\n",
        "            layer.set_weights([new_weights, biases])\n",
        "\n",
        "    return model\n",
        "\n",
        "# 가지치기된 모델\n",
        "pruned_model = prune_weights(model, pruning_percentage=0.8)"
      ],
      "metadata": {
        "id": "0fpGftLRATRD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가지치기된 모델 재훈련\n",
        "pruned_model.compile(optimizer='adam',\n",
        "                     loss='categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "pruned_model.fit(X_train, y_train_categorical, epochs=5, batch_size=32, validation_data=(X_val, y_val_categorical))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rB_6dgvATTr",
        "outputId": "4e7793eb-410d-47d8-c073-8057251d36f3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "219/219 [==============================] - 76s 342ms/step - loss: 0.2653 - accuracy: 0.8834 - val_loss: 0.0184 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "219/219 [==============================] - 74s 339ms/step - loss: 0.2266 - accuracy: 0.9066 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "219/219 [==============================] - 75s 343ms/step - loss: 0.2078 - accuracy: 0.9219 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "219/219 [==============================] - 75s 341ms/step - loss: 0.2205 - accuracy: 0.9159 - val_loss: 0.0261 - val_accuracy: 0.9992\n",
            "Epoch 5/5\n",
            "219/219 [==============================] - 75s 343ms/step - loss: 0.1783 - accuracy: 0.9315 - val_loss: 0.0082 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x780fc0470340>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가지치기된 모델의 가중치 저장 (희소성 적용)\n",
        "def save_sparse_model(model, file_path):\n",
        "    with h5py.File(file_path, 'w') as f:\n",
        "        for layer in model.layers:\n",
        "            if isinstance(layer, Dense) or isinstance(layer, Conv1D):\n",
        "                weights, biases = layer.get_weights()\n",
        "                # 0이 아닌 가중치만 저장\n",
        "                non_zero_indices = weights != 0\n",
        "                non_zero_weights = weights[non_zero_indices]\n",
        "                f.create_dataset(layer.name + '_weights', data=non_zero_weights)\n",
        "                f.create_dataset(layer.name + '_biases', data=biases)\n",
        "                f.create_dataset(layer.name + '_non_zero_indices', data=non_zero_indices)\n",
        "            else:\n",
        "                # 다른 레이어는 그대로 저장\n",
        "                weights = layer.get_weights()\n",
        "                for i, weight in enumerate(weights):\n",
        "                    f.create_dataset(layer.name + f'_weights_{i}', data=weight)\n",
        "\n",
        "# 희소성 적용한 모델 저장\n",
        "save_sparse_model(pruned_model, 'model_after_pruning_sparse.h5')\n",
        "\n",
        "# 희소성 적용 모델 로드 함수\n",
        "def load_sparse_model(file_path, original_model):\n",
        "    with h5py.File(file_path, 'r') as f:\n",
        "        for layer in original_model.layers:\n",
        "            if isinstance(layer, Dense) or isinstance(layer, Conv1D):\n",
        "                non_zero_weights = f[layer.name + '_weights'][:]\n",
        "                biases = f[layer.name + '_biases'][:]\n",
        "                non_zero_indices = f[layer.name + '_non_zero_indices'][:]\n",
        "                original_weights = np.zeros(layer.get_weights()[0].shape)\n",
        "                original_weights[non_zero_indices] = non_zero_weights\n",
        "                layer.set_weights([original_weights, biases])\n",
        "            else:\n",
        "                weights = []\n",
        "                i = 0\n",
        "                while f'{layer.name}_weights_{i}' in f:\n",
        "                    weights.append(f[layer.name + f'_weights_{i}'][:])\n",
        "                    i += 1\n",
        "                layer.set_weights(weights)\n",
        "\n",
        "# 희소성 적용 모델 로드\n",
        "model_for_evaluation = create_model()\n",
        "load_sparse_model('model_after_pruning_sparse.h5', model_for_evaluation)"
      ],
      "metadata": {
        "id": "AZlKF33jCVMi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 컴파일\n",
        "model_for_evaluation.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "4EQ3out2adv2"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 크기 비교\n",
        "original_model_size = os.path.getsize('model.h5') / (1024 * 1024)\n",
        "pruned_model_size = os.path.getsize('model_after_pruning.h5') / (1024 * 1024)\n",
        "sparse_model_size = os.path.getsize('model_after_pruning_sparse.h5') / (1024 * 1024)\n",
        "print(f\"Original Model Size: {original_model_size:.2f} MB\")\n",
        "print(f\"Pruned Model Size (without sparsity): {pruned_model_size:.2f} MB\")\n",
        "print(f\"Sparse Pruned Model Size: {sparse_model_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2UyFFvSC4WY",
        "outputId": "e8894f44-daa1-487f-9a6c-d0a8743149e7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Size: 439.50 MB\n",
            "Pruned Model Size (without sparsity): 439.50 MB\n",
            "Sparse Pruned Model Size: 68.73 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 메모리 사용량 확인\n",
        "process = psutil.Process(os.getpid())\n",
        "memory_usage_after = process.memory_info().rss / (1024 * 1024)  # 메모리 사용량을 MB 단위로 변환\n",
        "print(f\"Memory Usage (MB): {memory_usage_after:.2f}\")\n",
        "\n",
        "# 가지치기된 모델 추론 시간 측정\n",
        "start_time = time.time()\n",
        "y_pred_categorical_after = model_for_evaluation.predict(X_test)\n",
        "end_time = time.time()\n",
        "inference_time_after = end_time - start_time\n",
        "\n",
        "print(f\"Inference Time (s) : {inference_time_after:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bbv9_5FoATcX",
        "outputId": "83bfee40-d98f-4e29-f90d-86abd49955ed"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Usage (MB): 12836.93\n",
            "40/40 [==============================] - 3s 69ms/step\n",
            "Inference Time (s) : 2.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 검증 데이터 평가\n",
        "val_loss_after, val_accuracy_after = model_for_evaluation.evaluate(X_val, y_val_categorical)\n",
        "print(f\"Validation Loss: {val_loss_after:4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy_after:4f}\")\n",
        "\n",
        "# 테스트 데이터 평가\n",
        "test_loss_after, test_accuracy_after = model_for_evaluation.evaluate(X_test, y_test_categorical)\n",
        "print(f\"test Loss: {test_loss_after:4f}\")\n",
        "print(f\"testn Accuracy: {test_accuracy_after:4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NSb0r-zDBnu",
        "outputId": "62d32336-c63d-4517-d2a1-6c354a4f514e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 3s 71ms/step - loss: 0.0082 - accuracy: 1.0000\n",
            "Validation Loss: 0.008229\n",
            "Validation Accuracy: 1.000000\n",
            "40/40 [==============================] - 3s 72ms/step - loss: 0.0182 - accuracy: 1.0000\n",
            "test Loss: 0.018203\n",
            "testn Accuracy: 1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 혼동 행렬 및 성능 지표 출력 (테스트 데이터)\n",
        "y_pred_after = np.argmax(y_pred_categorical_after, axis=1)\n",
        "conf_matrix_test_after = confusion_matrix(y_test_encoded, y_pred_after)\n",
        "class_report_test_after = classification_report(y_test_encoded, y_pred_after, target_names=categories)\n",
        "f1_score_after = f1_score(y_test_encoded, y_pred_after, average='weighted')\n",
        "\n",
        "# 성능 비교 출력\n",
        "performance_comparison = {\n",
        "    'Metric': ['Model Size (MB)', 'Memory Usage (MB)', 'Inference Time (s)', 'Validation Loss', 'Validation Accuracy', 'Test Loss', 'Test Accuracy', 'F1 Score'],\n",
        "    'After Pruning': [sparse_model_size, memory_usage_after, inference_time_after, val_loss_after, val_accuracy_after, test_loss_after, test_accuracy_after, f1_score_after]\n",
        "}\n",
        "\n",
        "import pandas as pd\n",
        "performance_df = pd.DataFrame(performance_comparison)\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "print(performance_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZphnCsXCDLoQ",
        "outputId": "a2cc1fa7-000b-4552-f73b-42e379a4ba4e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performance Comparison:\n",
            "                Metric  After Pruning\n",
            "0      Model Size (MB)      68.725109\n",
            "1    Memory Usage (MB)   12836.925781\n",
            "2   Inference Time (s)       2.986383\n",
            "3      Validation Loss       0.008229\n",
            "4  Validation Accuracy       1.000000\n",
            "5            Test Loss       0.018203\n",
            "6        Test Accuracy       1.000000\n",
            "7             F1 Score       1.000000\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "10WFvJea2sDZlxOpclidXSus0XkisxzKw",
      "authorship_tag": "ABX9TyPWuufKecMhLSEWx/X9NlaE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheCaveOfAdullam/study2/blob/main/marchine6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDdteyWqUYgY",
        "outputId": "5e0a77b1-9674-49b1-f925-12771b876d55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-model-optimization\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (0.1.8)\n",
            "Requirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.25.2)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.16.0)\n",
            "Installing collected packages: tensorflow-model-optimization\n",
            "Successfully installed tensorflow-model-optimization-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-model-optimization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow_model_optimization as tfmot\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import psutil"
      ],
      "metadata": {
        "id": "QfmXCkuxWYXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the directory path\n",
        "base_path = '/content/drive/MyDrive/marine_data'\n",
        "os.chdir(base_path)\n",
        "\n",
        "# Function to resize images\n",
        "def resize_img(img, size):\n",
        "    return img.resize(size)\n",
        "\n",
        "# Function to load and resize images from a directory\n",
        "def load_img(file_path):\n",
        "    data = []\n",
        "    full_path = os.path.join(base_path, file_path)\n",
        "    for f in os.listdir(full_path):\n",
        "        data.append(resize_img(Image.open(os.path.join(full_path, f)), (64, 64)))\n",
        "    return data"
      ],
      "metadata": {
        "id": "G_ev8l_ZWYZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load images for different sets\n",
        "train_normal = load_img('train/normal')\n",
        "test_normal = load_img('test/normal')\n",
        "val_normal = load_img('validation/normal')\n",
        "\n",
        "train_fault_BB = load_img('train/fault_BB')\n",
        "test_fault_BB = load_img('test/fault_BB')\n",
        "val_fault_BB = load_img('validation/fault_BB')\n",
        "\n",
        "train_fault_SM = load_img('train/fault_SM')\n",
        "test_fault_SM = load_img('test/fault_SM')\n",
        "val_fault_SM = load_img('validation/fault_SM')\n",
        "\n",
        "train_fault_RI = load_img('train/fault_RI')\n",
        "test_fault_RI = load_img('test/fault_RI')\n",
        "val_fault_RI = load_img('validation/fault_RI')"
      ],
      "metadata": {
        "id": "H6TNySi1WYct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the lengths of the datasets\n",
        "print(len(train_normal), len(test_normal), len(val_normal))\n",
        "print(len(train_fault_BB), len(test_fault_BB), len(val_fault_BB))\n",
        "print(len(train_fault_SM), len(test_fault_SM), len(val_fault_SM))\n",
        "print(len(train_fault_RI), len(test_fault_RI), len(val_fault_RI))\n",
        "\n",
        "# Function to convert images to arrays and normalize\n",
        "def img_to_array(img):\n",
        "    return np.array(img, dtype='float32') / 255.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC6F5_pYWYfF",
        "outputId": "82fe759b-edee-41e3-fc09-28851c4ade2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000 3000 1500\n",
            "3004 3000 1500\n",
            "3000 3000 1500\n",
            "3000 3000 1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert images to arrays and define labels\n",
        "train_normal_arr = np.array([img_to_array(normal) for normal in train_normal])\n",
        "test_normal_arr = np.array([img_to_array(normal) for normal in test_normal])\n",
        "val_normal_arr = np.array([img_to_array(normal) for normal in val_normal])\n",
        "\n",
        "train_fault_BB_arr = np.array([img_to_array(fault) for fault in train_fault_BB])\n",
        "test_fault_BB_arr = np.array([img_to_array(fault) for fault in test_fault_BB])\n",
        "val_fault_BB_arr = np.array([img_to_array(fault) for fault in val_fault_BB])\n",
        "\n",
        "train_fault_SM_arr = np.array([img_to_array(fault) for fault in train_fault_SM])\n",
        "test_fault_SM_arr = np.array([img_to_array(fault) for fault in test_fault_SM])\n",
        "val_fault_SM_arr = np.array([img_to_array(fault) for fault in val_fault_SM])\n",
        "\n",
        "train_fault_RI_arr = np.array([img_to_array(fault) for fault in train_fault_RI])\n",
        "test_fault_RI_arr = np.array([img_to_array(fault) for fault in test_fault_RI])\n",
        "val_fault_RI_arr = np.array([img_to_array(fault) for fault in val_fault_RI])\n",
        "\n",
        "train_normal_sol = np.array([0] * len(train_normal))\n",
        "test_normal_sol = np.array([0] * len(test_normal))\n",
        "val_normal_sol = np.array([0] * len(val_normal))\n",
        "\n",
        "train_fault_BB_sol = np.array([1] * len(train_fault_BB))\n",
        "test_fault_BB_sol = np.array([1] * len(test_fault_BB))\n",
        "val_fault_BB_sol = np.array([1] * len(val_fault_BB))\n",
        "\n",
        "train_fault_SM_sol = np.array([2] * len(train_fault_SM))\n",
        "test_fault_SM_sol = np.array([2] * len(test_fault_SM))\n",
        "val_fault_SM_sol = np.array([2] * len(val_fault_SM))\n",
        "\n",
        "train_fault_RI_sol = np.array([3] * len(train_fault_RI))\n",
        "test_fault_RI_sol = np.array([3] * len(test_fault_RI))\n",
        "val_fault_RI_sol = np.array([3] * len(val_fault_RI))"
      ],
      "metadata": {
        "id": "8qDLOAl-WYiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine data and labels\n",
        "train_img = np.concatenate((train_normal_arr, train_fault_BB_arr, train_fault_SM_arr, train_fault_RI_arr))\n",
        "train_sol = np.concatenate((train_normal_sol, train_fault_BB_sol, train_fault_SM_sol, train_fault_RI_sol))\n",
        "\n",
        "test_img = np.concatenate((test_normal_arr, test_fault_BB_arr, test_fault_SM_arr, test_fault_RI_arr))\n",
        "test_sol = np.concatenate((test_normal_sol, test_fault_BB_sol, test_fault_SM_sol, test_fault_RI_sol))\n",
        "\n",
        "val_img = np.concatenate((val_normal_arr, val_fault_BB_arr, val_fault_SM_arr, val_fault_RI_arr))\n",
        "val_sol = np.concatenate((val_normal_sol, val_fault_BB_sol, val_fault_SM_sol, val_fault_RI_sol))\n",
        "\n",
        "# One-hot encode the labels\n",
        "train_sol = to_categorical(train_sol, 4)\n",
        "test_sol = to_categorical(test_sol, 4)\n",
        "val_sol = to_categorical(val_sol, 4)"
      ],
      "metadata": {
        "id": "GuNBKegAWYkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the original model\n",
        "model = Sequential()\n",
        "\n",
        "# First block\n",
        "model.add(BatchNormalization(input_shape=train_img.shape[1:]))\n",
        "model.add(Conv2D(filters=64, kernel_size=(5, 5), padding='same', activation='elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Second block\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (5, 5), padding='same', activation='elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Third block\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, (5, 5), padding='same', activation='elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='elu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='elu'))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "# Compile the original model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=1e-3), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "wV052C30WYnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply pruning\n",
        "pruning_params = {\n",
        "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
        "        initial_sparsity=0.0, final_sparsity=0.50, begin_step=0, end_step=np.ceil(len(train_img) / 128).astype(np.int32) * 10)\n",
        "}\n",
        "\n",
        "pruned_layers = []\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.Dense):\n",
        "        pruned_layers.append(tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params))\n",
        "    else:\n",
        "        pruned_layers.append(layer)\n",
        "\n",
        "pruned_model = Sequential(pruned_layers)\n",
        "\n",
        "pruned_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=1e-3), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "R14PxojxWYp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the pruned model\n",
        "callbacks = [tfmot.sparsity.keras.UpdatePruningStep()]\n",
        "\n",
        "pruned_model.fit(x=train_img, y=train_sol, batch_size=128, epochs=10, verbose=1, validation_data=(val_img, val_sol), callbacks=callbacks)"
      ],
      "metadata": {
        "id": "eqIMi1SgWYsM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b4c4396-bd2a-48d9-e739-3c4b1e93d7f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "94/94 [==============================] - 1126s 12s/step - loss: 0.1597 - accuracy: 0.9763 - val_loss: 3.6428 - val_accuracy: 0.7492\n",
            "Epoch 2/30\n",
            "94/94 [==============================] - 1041s 11s/step - loss: 0.0392 - accuracy: 0.9983 - val_loss: 1.9868e-10 - val_accuracy: 1.0000\n",
            "Epoch 3/30\n",
            "94/94 [==============================] - 1065s 11s/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 1.9868e-11 - val_accuracy: 1.0000\n",
            "Epoch 4/30\n",
            "94/94 [==============================] - 1024s 11s/step - loss: 0.0430 - accuracy: 0.9991 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 5/30\n",
            "94/94 [==============================] - 1032s 11s/step - loss: 0.0126 - accuracy: 0.9996 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 6/30\n",
            "94/94 [==============================] - 1020s 11s/step - loss: 0.0172 - accuracy: 0.9997 - val_loss: 0.0209 - val_accuracy: 0.9997\n",
            "Epoch 7/30\n",
            "94/94 [==============================] - 1030s 11s/step - loss: 0.7939 - accuracy: 0.9956 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 8/30\n",
            "94/94 [==============================] - 1025s 11s/step - loss: 0.1146 - accuracy: 0.9993 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 9/30\n",
            "94/94 [==============================] - 1015s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 10/30\n",
            "94/94 [==============================] - 1018s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 11/30\n",
            "94/94 [==============================] - 1018s 11s/step - loss: 0.0023 - accuracy: 0.9999 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 12/30\n",
            "94/94 [==============================] - 1017s 11s/step - loss: 0.0032 - accuracy: 0.9999 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 13/30\n",
            "94/94 [==============================] - 1020s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 14/30\n",
            "94/94 [==============================] - 1018s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 15/30\n",
            "94/94 [==============================] - 1024s 11s/step - loss: 0.0050 - accuracy: 0.9999 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 16/30\n",
            "94/94 [==============================] - 1021s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 17/30\n",
            "94/94 [==============================] - 1021s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 18/30\n",
            "94/94 [==============================] - 1016s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 19/30\n",
            "94/94 [==============================] - 993s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 20/30\n",
            "94/94 [==============================] - 1019s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 21/30\n",
            "94/94 [==============================] - 1022s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 22/30\n",
            "94/94 [==============================] - 1024s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 23/30\n",
            "94/94 [==============================] - 1019s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 24/30\n",
            "94/94 [==============================] - 1016s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 25/30\n",
            "94/94 [==============================] - 995s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 26/30\n",
            "94/94 [==============================] - 1020s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 27/30\n",
            "94/94 [==============================] - 1021s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 28/30\n",
            "94/94 [==============================] - 1021s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 29/30\n",
            "94/94 [==============================] - 1014s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 30/30\n",
            "94/94 [==============================] - 1013s 11s/step - loss: 0.1606 - accuracy: 0.9996 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e06f410c6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Strip pruning wrappers from the model\n",
        "pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)"
      ],
      "metadata": {
        "id": "vMaTApELS8xG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save models\n",
        "model.save('original_model.h5')\n",
        "pruned_model.save('pruned_model.h5')"
      ],
      "metadata": {
        "id": "Pytkpw8rWYuu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bb84ee7-8ac1-45b8-e4cb-d563f5202353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model size\n",
        "original_size = os.path.getsize('original_model.h5')\n",
        "pruned_size = os.path.getsize('pruned_model.h5')\n",
        "print(f\"Original Model Size: {original_size / 1024:.2f} KB\")\n",
        "print(f\"Pruned Model Size: {pruned_size / 1024:.2f} KB\")"
      ],
      "metadata": {
        "id": "qdeoDSd5WYxc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcf703cb-c3e1-4180-88f8-d8f51013b061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Size: 20601.56 KB\n",
            "Pruned Model Size: 61721.56 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference time measurement function\n",
        "def measure_inference_time(model, data, num_runs=100):\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        model.predict(data)\n",
        "    end_time = time.time()\n",
        "    return (end_time - start_time) / num_runs"
      ],
      "metadata": {
        "id": "odby105CWY0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure inference time\n",
        "original_inference_time = measure_inference_time(model, test_img)\n",
        "pruned_inference_time = measure_inference_time(pruned_model, test_img)\n",
        "print(f\"Original Model Inference Time: {original_inference_time:.6f} seconds per sample\")\n",
        "print(f\"Pruned Model Inference Time: {pruned_inference_time:.6f} seconds per sample\")"
      ],
      "metadata": {
        "id": "k5NFq68XWY2U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2e0afe6-0e5d-456a-a642-5673aac4c6e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "375/375 [==============================] - 246s 652ms/step\n",
            "375/375 [==============================] - 233s 622ms/step\n",
            "375/375 [==============================] - 235s 627ms/step\n",
            "295/375 [======================>.......] - ETA: 49s"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory usage measurement function\n",
        "def measure_memory_usage(model, data):\n",
        "    process = psutil.Process(os.getpid())\n",
        "    start_memory = process.memory_info().rss\n",
        "    model.predict(data)\n",
        "    end_memory = process.memory_info().rss\n",
        "    return (end_memory - start_memory) / (1024 * 1024)  # in MB"
      ],
      "metadata": {
        "id": "oOj9xte_XW_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure memory usage\n",
        "original_memory_usage = measure_memory_usage(model, test_img)\n",
        "pruned_memory_usage = measure_memory_usage(pruned_model, test_img)\n",
        "print(f\"Original Model Memory Usage: {original_memory_usage:.2f} MB\")\n",
        "print(f\"Pruned Model Memory Usage: {pruned_memory_usage:.2f} MB\")"
      ],
      "metadata": {
        "id": "w2qo_VzqXXCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the pruned model on test data\n",
        "loss, accuracy = pruned_model.evaluate(test_img, test_sol, batch_size=128)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "s_13B3EaXXEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the labels\n",
        "test_pred = pruned_model.predict(test_img)\n",
        "test_pred_classes = np.argmax(test_pred, axis=1)\n",
        "test_true_classes = np.argmax(test_sol, axis=1)"
      ],
      "metadata": {
        "id": "IPfgTnvvXXG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(test_true_classes, test_pred_classes)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "TbbHyvw0XXJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "class_report = classification_report(test_true_classes, test_pred_classes, target_names=['Normal', 'Fault_BB', 'Fault_SM', 'Fault_RI'])\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "id": "ZXVGPc-qXXMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(4)\n",
        "plt.xticks(tick_marks, ['Normal', 'Fault_BB', 'Fault_SM', 'Fault_RI'], rotation=45)\n",
        "plt.yticks(tick_marks, ['Normal', 'Fault_BB', 'Fault_SM', 'Fault_RI'])\n",
        "\n",
        "fmt = 'd'\n",
        "thresh = conf_matrix.max() / 2.\n",
        "for i, j in np.ndindex(conf_matrix.shape):\n",
        "    plt.text(j, i, format(conf_matrix[i, j], fmt),\n",
        "             ha=\"center\", va=\"center\",\n",
        "             color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "91Tv5CmAXXOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eXLKkNFTXXRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NkU9xYPuXXUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t_syEiT_WY44"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
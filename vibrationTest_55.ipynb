{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMi5ZWIKbdZ6d2dr3ZMDW7+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheCaveOfAdullam/study2/blob/main/vibrationTest_55.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNCv1nawwVyN",
        "outputId": "bc7e64d9-fd83-457a-a886-3639558098b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_model_optimization\n",
        "!pip install h5py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EgzBhU0wdEx",
        "outputId": "6b1fedc3-28df-4b99-ddf7-b9890b39ee31"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_model_optimization in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (0.1.8)\n",
            "Requirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (1.25.2)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (1.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.11.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import psutil\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "import tensorflow_model_optimization as tfmot\n",
        "import h5py\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "dDAIS9HQwdHg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 경로 설정\n",
        "base_dir = '/content/drive/MyDrive/ship_data'\n",
        "categories = ['normal', 'fault_BB', 'fault_RI', 'fault_SM']\n",
        "\n",
        "# 데이터 로드 및 전처리 함수 정의\n",
        "def load_data(base_dir, split):\n",
        "    X = []\n",
        "    y = []\n",
        "    split_dir = os.path.join(base_dir, split)\n",
        "    for category in categories:\n",
        "        category_dir = os.path.join(split_dir, category)\n",
        "        for file in os.listdir(category_dir):\n",
        "            file_path = os.path.join(category_dir, file)\n",
        "            data = pd.read_csv(file_path, header=None).values\n",
        "            data = pd.to_numeric(data.flatten(), errors='coerce').reshape(-1, data.shape[1])\n",
        "            data = np.nan_to_num(data).astype('float32')  # NaN 값을 0으로 대체하고, float32로 변환\n",
        "            X.append(data)\n",
        "            y.append(category)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# 데이터 로드\n",
        "X_train, y_train = load_data(base_dir, 'train')\n",
        "X_val, y_val = load_data(base_dir, 'validation')\n",
        "X_test, y_test = load_data(base_dir, 'test')\n",
        "\n",
        "# 데이터 차원 변경 (CNN 입력 형식에 맞게)\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_val = np.expand_dims(X_val, axis=-1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "\n",
        "# 레이블 인코딩\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_val_encoded = label_encoder.transform(y_val)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# 원-핫 인코딩\n",
        "y_train_categorical = to_categorical(y_train_encoded)\n",
        "y_val_categorical = to_categorical(y_val_encoded)\n",
        "y_test_categorical = to_categorical(y_test_encoded)"
      ],
      "metadata": {
        "id": "fhomGtk8wdKB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 CNN 모델 정의\n",
        "def create_model():\n",
        "    model = Sequential(name='CNN_Model')\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu',\n",
        "                     input_shape=(X_train.shape[1], X_train.shape[2]),\n",
        "                     kernel_regularizer=l2(0.001), name='conv1'))\n",
        "    model.add(MaxPooling1D(pool_size=2, name='maxpool1'))\n",
        "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu',\n",
        "                     kernel_regularizer=l2(0.001), name='conv2'))\n",
        "    model.add(MaxPooling1D(pool_size=2, name='maxpool2'))\n",
        "    model.add(Flatten(name='flatten'))\n",
        "    model.add(Dense(100, activation='relu', kernel_regularizer=l2(0.0000001), name='dense1'))\n",
        "    model.add(Dropout(0.5, name='dropout'))\n",
        "    model.add(Dense(len(categories), activation='softmax', name='output'))\n",
        "    return model\n",
        "\n",
        "# Create and compile the model\n",
        "model = create_model()\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "_tWHrGbfwdND"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X_train, y_train_categorical, epochs=5, batch_size=32, validation_data=(X_val, y_val_categorical))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLpTCohAwdP6",
        "outputId": "3b7e759a-71ed-4a5b-da58-4a7e9387ec14"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "219/219 [==============================] - 80s 362ms/step - loss: 1.4542 - accuracy: 0.4842 - val_loss: 1.1796 - val_accuracy: 0.5869\n",
            "Epoch 2/5\n",
            "219/219 [==============================] - 79s 360ms/step - loss: 1.2877 - accuracy: 0.4997 - val_loss: 1.1957 - val_accuracy: 0.5869\n",
            "Epoch 3/5\n",
            "219/219 [==============================] - 78s 358ms/step - loss: 1.2732 - accuracy: 0.4997 - val_loss: 1.1777 - val_accuracy: 0.5869\n",
            "Epoch 4/5\n",
            "219/219 [==============================] - 78s 357ms/step - loss: 1.2670 - accuracy: 0.4997 - val_loss: 1.1779 - val_accuracy: 0.5869\n",
            "Epoch 5/5\n",
            "219/219 [==============================] - 79s 361ms/step - loss: 1.0586 - accuracy: 0.5540 - val_loss: 0.6708 - val_accuracy: 0.7246\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e6bdc19e320>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 원래 모델 저장\n",
        "model.save('model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R6u9rDfwdV5",
        "outputId": "54a35f32-0009-4f7f-824f-38bbdd894fa1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가지치기 함수 정의\n",
        "def prune_weights(model, pruning_percentage=0.8):\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, Dense):\n",
        "            weights, biases = layer.get_weights()\n",
        "\n",
        "            # 가중치의 절대값 기준으로 임계값 계산\n",
        "            abs_weights = np.abs(weights)\n",
        "            threshold = np.percentile(abs_weights, pruning_percentage * 100)\n",
        "\n",
        "            # 임계값 이하의 가중치를 0으로 설정\n",
        "            new_weights = np.where(abs_weights < threshold, 0, weights)\n",
        "\n",
        "            # 가지치기된 가중치로 레이어 설정\n",
        "            layer.set_weights([new_weights, biases])\n",
        "\n",
        "    return model\n",
        "\n",
        "# 가지치기된 모델\n",
        "pruned_model = prune_weights(model, pruning_percentage=0.8)"
      ],
      "metadata": {
        "id": "kD6CXHY8wdYx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가지치기된 모델 재훈련\n",
        "pruned_model.compile(optimizer='adam',\n",
        "                     loss='categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "pruned_model.fit(X_train, y_train_categorical, epochs=5, batch_size=32, validation_data=(X_val, y_val_categorical))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EkPjp72wdbZ",
        "outputId": "43213f03-b543-4a7c-c0a8-1a1a73fd749b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "219/219 [==============================] - 80s 359ms/step - loss: 0.7006 - accuracy: 0.6855 - val_loss: 0.2958 - val_accuracy: 0.9225\n",
            "Epoch 2/5\n",
            "219/219 [==============================] - 78s 357ms/step - loss: 0.5426 - accuracy: 0.7491 - val_loss: 0.9047 - val_accuracy: 0.7629\n",
            "Epoch 3/5\n",
            "219/219 [==============================] - 80s 365ms/step - loss: 0.5681 - accuracy: 0.7691 - val_loss: 0.2034 - val_accuracy: 0.9890\n",
            "Epoch 4/5\n",
            "219/219 [==============================] - 81s 369ms/step - loss: 0.4938 - accuracy: 0.7935 - val_loss: 0.2466 - val_accuracy: 0.9280\n",
            "Epoch 5/5\n",
            "219/219 [==============================] - 80s 367ms/step - loss: 0.4457 - accuracy: 0.7968 - val_loss: 0.1846 - val_accuracy: 0.9687\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e6b646189a0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가지치기된 모델의 가중치 저장 (희소성 적용)\n",
        "def save_sparse_model(model, file_path):\n",
        "    with h5py.File(file_path, 'w') as f:\n",
        "        for layer in model.layers:\n",
        "            if isinstance(layer, Dense) or isinstance(layer, Conv1D):\n",
        "                weights, biases = layer.get_weights()\n",
        "                # 0이 아닌 가중치만 저장\n",
        "                non_zero_indices = weights != 0\n",
        "                non_zero_weights = weights[non_zero_indices]\n",
        "                f.create_dataset(layer.name + '_weights', data=non_zero_weights)\n",
        "                f.create_dataset(layer.name + '_biases', data=biases)\n",
        "                f.create_dataset(layer.name + '_non_zero_indices', data=non_zero_indices)\n",
        "            else:\n",
        "                # 다른 레이어는 그대로 저장\n",
        "                weights = layer.get_weights()\n",
        "                for i, weight in enumerate(weights):\n",
        "                    f.create_dataset(layer.name + f'_weights_{i}', data=weight)\n",
        "\n",
        "# 희소성 적용한 모델 저장\n",
        "save_sparse_model(pruned_model, 'model_after_pruning_sparse.h5')\n",
        "\n",
        "# 희소성 적용 모델 로드 함수\n",
        "def load_sparse_model(file_path, original_model):\n",
        "    with h5py.File(file_path, 'r') as f:\n",
        "        for layer in original_model.layers:\n",
        "            if isinstance(layer, Dense) or isinstance(layer, Conv1D):\n",
        "                non_zero_weights = f[layer.name + '_weights'][:]\n",
        "                biases = f[layer.name + '_biases'][:]\n",
        "                non_zero_indices = f[layer.name + '_non_zero_indices'][:]\n",
        "                original_weights = np.zeros(layer.get_weights()[0].shape)\n",
        "                original_weights[non_zero_indices] = non_zero_weights\n",
        "                layer.set_weights([original_weights, biases])\n",
        "            else:\n",
        "                weights = []\n",
        "                i = 0\n",
        "                while f'{layer.name}_weights_{i}' in f:\n",
        "                    weights.append(f[layer.name + f'_weights_{i}'][:])\n",
        "                    i += 1\n",
        "                layer.set_weights(weights)\n",
        "\n",
        "# 희소성 적용 모델 로드\n",
        "model_for_evaluation = create_model()\n",
        "load_sparse_model('model_after_pruning_sparse.h5', model_for_evaluation)\n",
        "\n",
        "# 모델 컴파일\n",
        "model_for_evaluation.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "va1HMjiMwdeZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 크기 비교\n",
        "original_model_size = os.path.getsize('model.h5') / (1024 * 1024)\n",
        "sparse_model_size = os.path.getsize('model_after_pruning_sparse.h5') / (1024 * 1024)\n",
        "print(f\"Original Model Size: {original_model_size:.2f} MB\")\n",
        "print(f\"Sparse Pruned Model Size: {sparse_model_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHPNBbPCwdhI",
        "outputId": "dff526d4-0b12-4eb9-fedc-24503d7bc020"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Size: 439.50 MB\n",
            "Sparse Pruned Model Size: 66.42 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 원본 모델 추론 시간 및 메모리 사용량 측정\n",
        "process = psutil.Process(os.getpid())\n",
        "memory_usage_before_inference_original = process.memory_info().rss / (1024 * 1024)\n",
        "\n",
        "start_time = time.time()\n",
        "y_pred_categorical_original = model.predict(X_test)\n",
        "end_time = time.time()\n",
        "inference_time_original = end_time - start_time\n",
        "\n",
        "memory_usage_after_inference_original = process.memory_info().rss / (1024 * 1024)\n",
        "\n",
        "print(f\"Original Model Memory Usage - Before: {memory_usage_before_inference_original:.2f} MB, After: {memory_usage_after_inference_original:.2f} MB\")\n",
        "print(f\"Original Model Inference Time: {inference_time_original:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "helmmUirxEpY",
        "outputId": "8dfc2bf2-13f8-4cb3-b1a3-32dffa0dc4a0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 3s 71ms/step\n",
            "Original Model Memory Usage - Before: 7843.52 MB, After: 8010.10 MB\n",
            "Original Model Inference Time: 3.14 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가지치기된 모델 추론 시간 및 메모리 사용량 측정\n",
        "memory_usage_before_inference_pruned = process.memory_info().rss / (1024 * 1024)\n",
        "\n",
        "start_time = time.time()\n",
        "y_pred_categorical_after = model_for_evaluation.predict(X_test)\n",
        "end_time = time.time()\n",
        "inference_time_pruned = end_time - start_time\n",
        "\n",
        "memory_usage_after_inference_pruned = process.memory_info().rss / (1024 * 1024)\n",
        "\n",
        "print(f\"Pruned Model Memory Usage - Before: {memory_usage_before_inference_pruned:.2f} MB, After: {memory_usage_after_inference_pruned:.2f} MB\")\n",
        "print(f\"Pruned Model Inference Time: {inference_time_pruned:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrd-iPuhxEsf",
        "outputId": "aa3aafb7-c5c3-4ab4-9eb1-fb80509e1fa7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 3s 72ms/step\n",
            "Pruned Model Memory Usage - Before: 8010.10 MB, After: 8080.45 MB\n",
            "Pruned Model Inference Time: 3.13 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 검증 데이터 평가\n",
        "val_loss_after, val_accuracy_after = model_for_evaluation.evaluate(X_val, y_val_categorical)\n",
        "print(f\"Validation Loss: {val_loss_after:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy_after:.4f}\")\n",
        "\n",
        "# 테스트 데이터 평가\n",
        "test_loss_after, test_accuracy_after = model_for_evaluation.evaluate(X_test, y_test_categorical)\n",
        "print(f\"Test Loss: {test_loss_after:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy_after:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMgIv-IVxEvR",
        "outputId": "33a33d56-33b9-4ca1-fd7f-24b6468a90f5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 3s 74ms/step - loss: 0.1846 - accuracy: 0.9687\n",
            "Validation Loss: 0.1846\n",
            "Validation Accuracy: 0.9687\n",
            "40/40 [==============================] - 3s 72ms/step - loss: 0.2023 - accuracy: 0.9491\n",
            "Test Loss: 0.2023\n",
            "Test Accuracy: 0.9491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터 평가\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test_categorical)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dq4blPxe4CUN",
        "outputId": "85447a7b-b942-4dc0-a872-bacf1b6eaa5e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 3s 77ms/step - loss: 0.2023 - accuracy: 0.9491\n",
            "Test Loss: 0.20227791368961334\n",
            "Test Accuracy: 0.9491392970085144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 혼동 행렬 및 성능 지표 출력 (테스트 데이터)\n",
        "y_pred_after = np.argmax(y_pred_categorical_after, axis=1)\n",
        "conf_matrix_test_after = confusion_matrix(y_test_encoded, y_pred_after)\n",
        "class_report_test_after = classification_report(y_test_encoded, y_pred_after, target_names=categories)\n",
        "f1_score_after = f1_score(y_test_encoded, y_pred_after, average='weighted')"
      ],
      "metadata": {
        "id": "62bvGHOexEyI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 성능 비교 출력\n",
        "performance_comparison = {\n",
        "    'Metric': ['Model Size (MB)', 'Original Memory Usage (MB)', 'Pruned Memory Usage (MB)', 'Original Inference Time (s)', 'Pruned Inference Time (s)', 'Validation Loss', 'Validation Accuracy', 'Test Loss', 'Test Accuracy', 'F1 Score'],\n",
        "    'Values': [sparse_model_size, memory_usage_after_inference_original, memory_usage_after_inference_pruned, inference_time_original, inference_time_pruned, val_loss_after, val_accuracy_after, test_loss_after, test_accuracy_after, f1_score_after]\n",
        "}\n",
        "\n",
        "performance_df = pd.DataFrame(performance_comparison)\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "print(performance_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bu4brQ_kxE1o",
        "outputId": "220197fa-3946-4013-bc01-668e741be883"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performance Comparison:\n",
            "                        Metric       Values\n",
            "0              Model Size (MB)    66.415867\n",
            "1   Original Memory Usage (MB)  8010.097656\n",
            "2     Pruned Memory Usage (MB)  8080.445312\n",
            "3  Original Inference Time (s)     3.135026\n",
            "4    Pruned Inference Time (s)     3.131386\n",
            "5              Validation Loss     0.184646\n",
            "6          Validation Accuracy     0.968701\n",
            "7                    Test Loss     0.202278\n",
            "8                Test Accuracy     0.949139\n",
            "9                     F1 Score     0.945770\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "usoum5vqwdkE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xl3QxJ6Xwdmo"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}